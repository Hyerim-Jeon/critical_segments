{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "620596b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "6b9c67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ruptures as rpt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "def segment_single_pid(pid, diff_dir=\"result\", shap_dir=\"shap_value_v1\", smooth_sigma=1, diff_k=1.0, penalty = 0.1, percentile_threshold = 80, shap_threshold = 0.0):\n",
    "    print(f\"Processing PID {pid}...\")\n",
    "\n",
    "    # === 1. Load diff data\n",
    "    diff_path = os.path.join(diff_dir, f\"{pid}_diff.csv\")\n",
    "    diff_df = pd.read_csv(diff_path)\n",
    "    mask = diff_df['difference'].isna()\n",
    "    default_guideline = 0.0\n",
    "    #score에서만 존재하고 annotation에 없어서 guideline value가 없는 경우 중립인 0으로 guideline 값 세팅\n",
    "    diff_df.loc[mask, 'difference'] = diff_df.loc[mask, 'feature_value'] - default_guideline\n",
    "    diff_df = diff_df[diff_df['feature'].between(1, 18)]  # ensure valid feature range\n",
    "\n",
    "    # Initialize full 0-filled DataFrame with shape (79, 18)\n",
    "    \n",
    "    ###chopin 79\n",
    "    #full_index = pd.Index(range(1, 80), name='measure')       # 1~79 measures\n",
    "        \n",
    "    ###beethoven 305\n",
    "    full_index = pd.Index(range(1, 306), name='measure')       # 1~305 measures\n",
    "    \n",
    "    full_columns = pd.Index(range(1, 19), name='feature')     # 1~18 features\n",
    "\n",
    "    # === Create diff_pivot: shape (79, 18), initialized with 0.0\n",
    "    diff_pivot = pd.DataFrame(0.0, index=full_index, columns=full_columns)\n",
    "    actual_diff = diff_df.pivot_table(index=\"measure\", columns=\"feature\", values=\"difference\", aggfunc='first')\n",
    "    diff_pivot.update(actual_diff)\n",
    "    \n",
    "    # diff_k 클수록 완만해짐\n",
    "    scaled_diff = diff_pivot.applymap(lambda x: np.tanh(x / diff_k))\n",
    "    diff_pivot.update(scaled_diff)\n",
    "    \n",
    "    # diff 값 절댓값으로 변환\n",
    "    diff_pivot = diff_pivot.applymap(np.abs)\n",
    "\n",
    "    # === Create critical_pivot: shape (79, 18), initialized with 0\n",
    "    critical_pivot = pd.DataFrame(0, index=full_index, columns=full_columns)\n",
    "    actual_critical = diff_df.pivot_table(index=\"measure\", columns=\"feature\", values=\"critical_score\", aggfunc='first')\n",
    "    critical_pivot.update(actual_critical)\n",
    "    \n",
    "    # === Scale critical_pivot values to 0 ~ 1\n",
    "    max_val = critical_pivot.to_numpy().max()\n",
    "    if max_val > 0:\n",
    "        critical_pivot = critical_pivot / max_val\n",
    "    else:\n",
    "        critical_pivot = pd.DataFrame(0.0, index=full_index, columns=full_columns)  # 모든 값이 0인 경우 처리\n",
    "\n",
    "\n",
    "    # Ensure all 18 features present\n",
    "    for pivot in [diff_pivot, critical_pivot]:\n",
    "        pivot = pivot.reindex(columns=range(1, 19), fill_value=0)\n",
    "        pivot.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "    # === 2. Load shap data\n",
    "    shap_path = os.path.join(shap_dir, f\"shap_{pid}.csv\")\n",
    "    shap_df = pd.read_csv(shap_path)\n",
    "    shap_df[\"measure\"] = range(1, len(shap_df) + 1)\n",
    "\n",
    "    # Melt shap, then pivot to same shape\n",
    "    shap_long = shap_df.melt(id_vars=\"measure\", var_name=\"feature_index\", value_name=\"shap_value\")\n",
    "    shap_long[\"feature\"] = shap_long[\"feature_index\"].astype(int) + 1\n",
    "    shap_pivot = shap_long.pivot_table(index=\"measure\", columns=\"feature\", values=\"shap_value\")\n",
    "    shap_pivot = shap_pivot.reindex(columns=range(1, 19), fill_value=0).sort_index()\n",
    "\n",
    "\n",
    "    common_measures = diff_pivot.index.intersection(shap_pivot.index)\n",
    "    diff_array = diff_pivot.loc[common_measures].values\n",
    "    shap_array = shap_pivot.loc[common_measures].values\n",
    "    critical_array = critical_pivot.loc[common_measures].fillna(0).values\n",
    "    \n",
    "    # 안전하게 NaN을 모두 0으로\n",
    "    diff_array = np.nan_to_num(diff_array, nan=0.0)\n",
    "    shap_array = np.nan_to_num(shap_array, nan=0.0)\n",
    "    critical_array = np.nan_to_num(critical_array, nan=0.0)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "    sns.heatmap(diff_pivot.values, cmap=\"coolwarm\", ax=axes[0], cbar=True)\n",
    "    axes[0].set_title(\"Difference\")\n",
    "    axes[0].set_xlabel(\"Feature\")\n",
    "    axes[0].set_ylabel(\"Measure\")\n",
    "\n",
    "    sns.heatmap(critical_pivot.values, cmap=\"YlOrRd\", ax=axes[1], cbar=True)\n",
    "    axes[1].set_title(\"Critical Score\")\n",
    "    axes[1].set_xlabel(\"Feature\")\n",
    "\n",
    "    sns.heatmap(shap_pivot.values, cmap=\"Blues\", ax=axes[2], cbar=True)\n",
    "    axes[2].set_title(\"SHAP Value\")\n",
    "    axes[2].set_xlabel(\"Feature\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # === 1. Calculate weighted_diff for all features\n",
    "    #weighted_diff = np.abs(diff_pivot.values * shap_pivot.values * critical_pivot.values)  # shape: (79, 18)\n",
    "    weighted_diff = np.abs(diff_pivot.values * critical_pivot.values)\n",
    "    \n",
    "    # 1. 먼저 log scaling -> 차이 극대화 위해\n",
    "    weighted_diff = np.log1p(weighted_diff)\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "    sns.heatmap(diff_pivot.values, cmap=\"coolwarm\", ax=axes[0], cbar=True)\n",
    "    axes[0].set_title(\"Difference\")\n",
    "    axes[0].set_xlabel(\"Feature\")\n",
    "    axes[0].set_ylabel(\"Measure\")\n",
    "\n",
    "    sns.heatmap(critical_pivot.values, cmap=\"YlOrRd\", ax=axes[1], cbar=True)\n",
    "    axes[1].set_title(\"Critical Score\")\n",
    "    axes[1].set_xlabel(\"Feature\")\n",
    "\n",
    "    sns.heatmap(weighted_diff, cmap=\"Blues\", ax=axes[2], cbar=True)\n",
    "    axes[2].set_title(\"Weighted Difference Value\")\n",
    "    axes[2].set_xlabel(\"Feature\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "    # === 2. Segment and plot each feature\n",
    "    n_features = weighted_diff.shape[1]\n",
    "    \n",
    "    change_points_list = []\n",
    "    score_list = []\n",
    "    smoothed_score_list = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(n_features, 1, figsize=(12, 3 * n_features), sharex=True)\n",
    "\n",
    "\n",
    "    for feature_idx in range(n_features):\n",
    "        ax = axes[feature_idx]\n",
    "\n",
    "        # Score per feature (1D time series)\n",
    "        score = weighted_diff[:, feature_idx]\n",
    "        smoothed_score = gaussian_filter1d(score, sigma=smooth_sigma)\n",
    "\n",
    "        # Save for return\n",
    "        score_list.append(score)\n",
    "        smoothed_score_list.append(smoothed_score)\n",
    "\n",
    "        # Segmentation\n",
    "        model = rpt.Pelt(model=\"rbf\").fit(smoothed_score.reshape(-1, 1))\n",
    "        change_points = model.predict(pen=penalty)\n",
    "        change_points_list.append(change_points)\n",
    "\n",
    "        # Plot\n",
    "        ax.plot(smoothed_score, label=\"Smoothed Score\")\n",
    "        for cp in change_points:\n",
    "            ax.axvline(cp, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "        ax.set_title(f\"Feature {feature_idx + 1}\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    plt.xlabel(\"Measure\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #각 feature에 대해 segment 별로 smoothed score값 평균을 이용해 상위 몇 percentile에 속하는 segment를 먼저 찾고, 그 segment들에 대해 shap value 평균을 구해서 threshold 이하일 경우 그 segment 정보를 출력하는 코드 이어서 작성\n",
    "    # 예: 상위 90% percentile 을 기준으로 중요 segment 선정\n",
    "    \n",
    "\n",
    "    # shap_array: (measure, feature) 크기 numpy 배열\n",
    "    # smoothed_score_list: feature별 (measure,) 크기 리스트\n",
    "    \n",
    "    selected_segments = []  # 결과를 담을 리스트\n",
    "\n",
    "    for feature_idx in range(n_features):\n",
    "        smoothed_scores = smoothed_score_list[feature_idx]\n",
    "        change_points = change_points_list[feature_idx]\n",
    "\n",
    "        # segment 경계 정의 (start, end)\n",
    "        segment_bounds = [(0, change_points[0])]\n",
    "        for i in range(1, len(change_points)):\n",
    "            segment_bounds.append((change_points[i-1], change_points[i]))\n",
    "\n",
    "        # 각 segment별 smoothed_score 평균 계산\n",
    "        segment_means = []\n",
    "        for start, end in segment_bounds:\n",
    "            segment_mean = smoothed_scores[start:end].mean()\n",
    "            segment_means.append(segment_mean)\n",
    "\n",
    "        segment_means = np.array(segment_means)\n",
    "\n",
    "        # 상위 percentile 임계값 계산\n",
    "        thresh_value = np.percentile(segment_means, percentile_threshold)\n",
    "\n",
    "        print(f\"\\nFeature {feature_idx+1} - 상위 {percentile_threshold} percentile threshold: {thresh_value:.4f}\")\n",
    "        \n",
    "\n",
    "\n",
    "        for idx, (start, end) in enumerate(segment_bounds):\n",
    "            segment_size = end - start\n",
    "            if segment_size > 20 or segment_size < 2:\n",
    "                continue  # 크기 20 초과 or 2 미만인 세그먼트는 무시\n",
    "\n",
    "            if segment_means[idx] >= thresh_value:\n",
    "                # 해당 segment 내 shap value 평균 계산\n",
    "                segment_shap_vals = shap_array[start:end, feature_idx]\n",
    "                shap_mean = segment_shap_vals.mean()\n",
    "                print(f\"  Segment {idx+1}: Measure {start+1} ~ {end}, \"\n",
    "                          f\"Smoothed Score Mean: {segment_means[idx]:.4f}, \")\n",
    "\n",
    "                \n",
    "                if shap_mean <= shap_threshold:\n",
    "                    print(f\"  Segment {idx+1}: Measure {start+1} ~ {end}, \"\n",
    "                          f\"Smoothed Score Mean: {segment_means[idx]:.4f}, \"\n",
    "                          f\"SHAP Mean: {shap_mean:.4f} (<= {shap_threshold})\")\n",
    "                    # 결과 저장\n",
    "                    segment_info = {\n",
    "                        \"feature\": feature_idx + 1,\n",
    "                        \"start_measure\": start + 1,\n",
    "                        \"end_measure\": end,\n",
    "                        \"smoothed_score_mean\": segment_means[idx],\n",
    "                        \"shap_mean\": shap_mean\n",
    "                    }\n",
    "                    selected_segments.append(segment_info)\n",
    "    \n",
    "    return selected_segments, smoothed_score_list, shap_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "f896c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_segment_pid(pid, selected_segments, diff_dir=\"result\",):\n",
    "\n",
    "    # === 1. Load diff data\n",
    "    diff_path = os.path.join(diff_dir, f\"{pid}_diff.csv\")\n",
    "    diff_df = pd.read_csv(diff_path)\n",
    "    mask = diff_df['difference'].isna()\n",
    "    default_guideline = 0.0\n",
    "    #score에서만 존재하고 annotation에 없어서 guideline value가 없는 경우 중립인 0으로 guideline 값 세팅\n",
    "    diff_df.loc[mask, 'difference'] = diff_df.loc[mask, 'feature_value'] - default_guideline\n",
    "    \n",
    "    \n",
    "    # === 2. Create empty list to collect filtered rows\n",
    "    result_rows = []\n",
    "\n",
    "    # === 3. Iterate over selected segments\n",
    "    for segment_id, seg in enumerate(selected_segments):\n",
    "        f = seg['feature']\n",
    "        start_m = seg['start_measure']\n",
    "        end_m = seg['end_measure']\n",
    "        score_mean = seg['smoothed_score_mean']\n",
    "        shap_mean = seg['shap_mean']\n",
    "\n",
    "        # Filter diff_df for this segment's feature and measure range\n",
    "        seg_df = diff_df[\n",
    "            (diff_df['feature'] == f) &\n",
    "            (diff_df['measure'] >= start_m) &\n",
    "            (diff_df['measure'] <= end_m)\n",
    "        ].copy()\n",
    "\n",
    "        # Add segment info\n",
    "        seg_df['segment'] = segment_id\n",
    "        seg_df['smoothed_score_mean'] = score_mean\n",
    "        seg_df['shap_mean'] = shap_mean\n",
    "\n",
    "        result_rows.append(seg_df)\n",
    "\n",
    "    # === 4. Concatenate all matched rows\n",
    "    if result_rows:\n",
    "        result_df = pd.concat(result_rows, ignore_index=True)\n",
    "    else:\n",
    "        result_df = pd.DataFrame()  # Empty DataFrame fallback\n",
    "\n",
    "    # === 5. Save result\n",
    "    result_path = os.path.join(diff_dir, f\"{pid}_result0.csv\")\n",
    "    result_df.to_csv(result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6503499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_segments(change_points_list, score_list):\n",
    "    all_sorted_segments = []\n",
    "\n",
    "    for feature_idx in range(len(score_list)):\n",
    "        cps = change_points_list[feature_idx]\n",
    "        score = score_list[feature_idx]\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        plt.plot(score, label=\"Weighted Diff Score\")\n",
    "        for cp in cps[:-1]:  # 마지막은 len(score)라 생략\n",
    "            plt.axvline(cp, color='red', linestyle='--', alpha=0.6)\n",
    "        plt.xlabel(\"Measure Index\")\n",
    "        plt.ylabel(\"Importance Score\")\n",
    "        plt.title(f\"Feature {feature_idx + 1}: SHAP-weighted Feature Difference\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Segment scoring\n",
    "        segment_scores = []\n",
    "        if len(cps) > 1:  # change_points가 하나 이상 있어야 세그먼트가 존재\n",
    "            for i in range(len(cps) - 1):\n",
    "                start, end = cps[i], cps[i+1]\n",
    "                segment_mean = score[start:end].mean()\n",
    "                segment_max = score[start:end].max()\n",
    "                segment_scores.append((i, start, end, segment_mean, segment_max))\n",
    "\n",
    "            # Sort by mean score\n",
    "            sorted_segments = sorted(segment_scores, key=lambda x: x[3], reverse=True)\n",
    "        else:\n",
    "            # No segments found, return an empty list\n",
    "            sorted_segments = []\n",
    "\n",
    "        all_sorted_segments.append(sorted_segments)\n",
    "\n",
    "        # Optional: print top-1 important segment if available\n",
    "        if sorted_segments:\n",
    "            print(f\"[Feature {feature_idx + 1}] Top segment (mean): {sorted_segments[0]}\")\n",
    "        else:\n",
    "            print(f\"[Feature {feature_idx + 1}] No segments found\")\n",
    "\n",
    "    return all_sorted_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "17a2c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_indices(score_list, percentile=70):\n",
    "    important_indices_list = []\n",
    "\n",
    "    for feature_idx, score in enumerate(score_list):\n",
    "        if len(score) == 0:\n",
    "            print(f\"[Feature {feature_idx + 1}] No scores available, skipping.\")\n",
    "            important_indices_list.append([])  # 빈 리스트를 추가\n",
    "            continue\n",
    "\n",
    "        threshold = np.percentile(score, percentile)\n",
    "        important_indices = np.where(score > threshold)[0]\n",
    "        important_indices_list.append(important_indices)\n",
    "\n",
    "        # Optional print\n",
    "        print(f\"[Feature {feature_idx + 1}] Threshold: {threshold:.4f}, Important Indices: {important_indices}\")\n",
    "\n",
    "    return important_indices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "bb899d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_segments(all_sorted_segments, percentile=70):\n",
    "    important_segments_list = []\n",
    "\n",
    "    for feature_idx, sorted_segments in enumerate(all_sorted_segments):\n",
    "        if not sorted_segments:\n",
    "            print(f\"[Feature {feature_idx + 1}] No segments to evaluate.\")\n",
    "            important_segments_list.append([])\n",
    "            continue\n",
    "\n",
    "        # Extract mean scores for thresholding\n",
    "        mean_scores = [seg[3] for seg in sorted_segments]\n",
    "        threshold = np.percentile(mean_scores, percentile)\n",
    "\n",
    "        # Filter segments above the threshold\n",
    "        important_segments = [seg for seg in sorted_segments if seg[3] > threshold]\n",
    "        important_segments_list.append(important_segments)\n",
    "\n",
    "        # Optional debug print\n",
    "        print(f\"[Feature {feature_idx + 1}] Threshold: {threshold:.4f}, \"\n",
    "              f\"Selected Segments: {[(s[1], s[2]) for s in important_segments]}\")\n",
    "\n",
    "    return important_segments_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "2ba6bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_result_pid(pid, base_csv='standard_c.csv', result_dir='result'):\n",
    "    # === Load CSVs\n",
    "    standard_df = pd.read_csv(base_csv)\n",
    "    result_df = pd.read_csv(os.path.join(result_dir, f\"{pid}_result0.csv\"))\n",
    "\n",
    "    # === Filter by pid\n",
    "    std_pid_df = standard_df[standard_df['pid'] == pid]\n",
    "    res_pid_df = result_df[result_df['pid'] == pid]\n",
    "\n",
    "    # === Get unique features in either standard or result\n",
    "    features = set(std_pid_df['feature'].unique()).union(res_pid_df['feature'].unique())\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for feature in sorted(features):\n",
    "        std_measures = set(std_pid_df[std_pid_df['feature'] == feature]['measure'])\n",
    "        res_measures = set(res_pid_df[res_pid_df['feature'] == feature]['measure'])\n",
    "\n",
    "        intersection = std_measures & res_measures\n",
    "        union = std_measures | res_measures\n",
    "\n",
    "        iou = len(intersection) / len(union) if union else 0\n",
    "        precision = len(intersection) / len(res_measures) if res_measures else 0\n",
    "        recall = len(intersection) / len(std_measures) if std_measures else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "        coverage = len(intersection) / len(std_measures) if std_measures else 0\n",
    "\n",
    "        results.append({\n",
    "            'pid': pid,\n",
    "            'feature': feature,\n",
    "            'standard_measures': sorted(std_measures),\n",
    "            'result_measures': sorted(res_measures),\n",
    "            'intersection': sorted(intersection),\n",
    "            'IoU': round(iou, 3),\n",
    "            'Precision': round(precision, 3),\n",
    "            'Recall': round(recall, 3),\n",
    "            'F1': round(f1, 3),\n",
    "            'Coverage': round(coverage, 3)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "3ca697e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare2_result_pid(pid, base_csv='standard2_c.csv', result_dir='result'):\n",
    "    # === Load CSVs\n",
    "    standard_df = pd.read_csv(base_csv)\n",
    "    result_df = pd.read_csv(os.path.join(result_dir, f\"{pid}_result0.csv\"))\n",
    "\n",
    "    # === Filter by pid\n",
    "    std_pid_df = standard_df[standard_df['pid'] == pid]\n",
    "    res_pid_df = result_df[result_df['pid'] == pid]\n",
    "\n",
    "    # === Set of measures (feature 무시)\n",
    "    std_measures = set(std_pid_df['measure'])\n",
    "    res_measures = set(res_pid_df['measure'])\n",
    "\n",
    "    intersection = std_measures & res_measures\n",
    "    union = std_measures | res_measures\n",
    "\n",
    "    iou = len(intersection) / len(union) if union else 0\n",
    "    precision = len(intersection) / len(res_measures) if res_measures else 0\n",
    "    recall = len(intersection) / len(std_measures) if std_measures else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "    coverage = len(intersection) / len(std_measures) if std_measures else 0\n",
    "\n",
    "    result = {\n",
    "        'pid': pid,\n",
    "        'standard_measures': sorted(std_measures),\n",
    "        'result_measures': sorted(res_measures),\n",
    "        'intersection': sorted(intersection),\n",
    "        'IoU': round(iou, 3),\n",
    "        'Precision': round(precision, 3),\n",
    "        'Recall': round(recall, 3),\n",
    "        'F1': round(f1, 3),\n",
    "        'Coverage': round(coverage, 3)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "d172fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_guide_pid(pid, standard_csv='standard_c.csv', result_dir='result'):\n",
    "    # Load standard and inter4 files\n",
    "    standard_df = pd.read_csv(standard_csv)\n",
    "    result_df = pd.read_csv(os.path.join(result_dir, f\"{pid}_inter4.csv\"))\n",
    "\n",
    "    # Filter by pid\n",
    "    std_df = standard_df[standard_df['pid'] == pid]\n",
    "    res_df = result_df.copy()\n",
    "\n",
    "    features = set(std_df['feature'].unique()).union(res_df['feature'].unique())\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for feature in sorted(features):\n",
    "        std_measures = set(std_df[std_df['feature'] == feature]['measure'])\n",
    "        res_measures = set(res_df[res_df['feature'] == feature]['measure'])\n",
    "\n",
    "        intersection = std_measures & res_measures\n",
    "        union = std_measures | res_measures\n",
    "\n",
    "        iou = len(intersection) / len(union) if union else 0\n",
    "        precision = len(intersection) / len(res_measures) if res_measures else 0\n",
    "        recall = len(intersection) / len(std_measures) if std_measures else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "        coverage = len(intersection) / len(std_measures) if std_measures else 0\n",
    "\n",
    "        results.append({\n",
    "            'pid': pid,\n",
    "            'feature': int(feature),\n",
    "            'standard_measures': sorted(std_measures),\n",
    "            'result_measures': sorted(res_measures),\n",
    "            'intersection': sorted(intersection),\n",
    "            'IoU': round(iou, 3),\n",
    "            'Precision': round(precision, 3),\n",
    "            'Recall': round(recall, 3),\n",
    "            'F1': round(f1, 3),\n",
    "            'Coverage': round(coverage, 3)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "8de76228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare2_guide_pid(pid, standard_csv='standard2_c.csv', result_dir='result'):\n",
    "    # Load standard and inter4 files\n",
    "    standard_df = pd.read_csv(standard_csv)\n",
    "    result_df = pd.read_csv(os.path.join(result_dir, f\"{pid}_inter4.csv\"))\n",
    "\n",
    "    # Filter by pid\n",
    "    std_df = standard_df[standard_df['pid'] == pid]\n",
    "    res_df = result_df.copy()\n",
    "    ###evaluation critical score chopin 3, beethoven 8부터\n",
    "    \n",
    "    if(standard_csv == 'standard2_c.csv'):\n",
    "        res_df = result_df[result_df['critical_score'] > 2]\n",
    "    else:\n",
    "        res_df = result_df[result_df['critical_score'] > 7]\n",
    "\n",
    "    std_measures = set(std_df['measure'])\n",
    "    res_measures = set(res_df['measure'])\n",
    "\n",
    "    intersection = std_measures & res_measures\n",
    "    union = std_measures | res_measures\n",
    "\n",
    "    iou = len(intersection) / len(union) if union else 0\n",
    "    precision = len(intersection) / len(res_measures) if res_measures else 0\n",
    "    recall = len(intersection) / len(std_measures) if std_measures else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "    coverage = len(intersection) / len(std_measures) if std_measures else 0\n",
    "\n",
    "    result = {\n",
    "        'pid': pid,\n",
    "        'standard_measures': sorted(std_measures),\n",
    "        'result_measures': sorted(res_measures),\n",
    "        'intersection': sorted(intersection),\n",
    "        'IoU': round(iou, 3),\n",
    "        'Precision': round(precision, 3),\n",
    "        'Recall': round(recall, 3),\n",
    "        'F1': round(f1, 3),\n",
    "        'Coverage': round(coverage, 3)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "3da27596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_pid(pid, result_dir='result', shap_dir='shap_v1'):\n",
    "    # Load result0.csv\n",
    "    result_path = os.path.join(result_dir, f\"{pid}_result0.csv\")\n",
    "    result_df = pd.read_csv(result_path)\n",
    "\n",
    "    # Load shap_{pid}.csv\n",
    "    shap_path = os.path.join(shap_dir, f\"shap_{pid}.csv\")\n",
    "    shap_df = pd.read_csv(shap_path)\n",
    "\n",
    "    # Make a copy to modify\n",
    "    updated_shap_df = shap_df.copy()\n",
    "\n",
    "    # Process each row in result0.csv\n",
    "    for _, row in result_df.iterrows():\n",
    "        measure = int(row['measure'])\n",
    "        feature = int(row['feature'])  # feature index: 1~19\n",
    "        value = (row['feature_value'] - row['difference'] + 1) * 7\n",
    "\n",
    "        # Column name in shap.csv is 'feature_{n}'\n",
    "        col_name = f'feature_{feature}'\n",
    "        \n",
    "        # Update shap value at that (measure, feature) location\n",
    "        if measure in updated_shap_df['measure'].values and col_name in updated_shap_df.columns:\n",
    "            updated_shap_df.loc[updated_shap_df['measure'] == measure, col_name] = value\n",
    "\n",
    "    # Save updated CSV\n",
    "    output_path = os.path.join(result_dir, f\"{pid}_improve.csv\")\n",
    "    updated_shap_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "4fa75b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def get_pid_list_from_result_diff(result_dir='result'):\n",
    "    pid_list = []\n",
    "    pattern = re.compile(r'^(\\d+)_diff\\.csv$')  # 패턴: 숫자_pid_diff.csv\n",
    "\n",
    "    for filename in os.listdir(result_dir):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            pid = int(match.group(1))\n",
    "            pid_list.append(pid)\n",
    "\n",
    "    return sorted(pid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "005c84d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Processing PID: 53\n",
      "\n",
      "🔄 Processing PID: 54\n",
      "\n",
      "🔄 Processing PID: 55\n",
      "\n",
      "🔄 Processing PID: 59\n",
      "\n",
      "🔄 Processing PID: 60\n"
     ]
    }
   ],
   "source": [
    "### chopin / beethoven (result 폴더에서 확인)\n",
    "# 먼저 처리할 pid 목록 정의\n",
    "pid_list = get_pid_list_from_result_diff(result_dir='result_v2_c')\n",
    "\n",
    "###chopin\n",
    "base='standard_c.csv'\n",
    "base2='standard2_c.csv'\n",
    "\n",
    "###beethoven\n",
    "#base='standard_b.csv'\n",
    "#base2='standard2_b.csv'\n",
    "\n",
    "# 결과 저장용 리스트 (필요 시)\n",
    "comparison_results = []\n",
    "comparison2_results = []\n",
    "guide_results = []\n",
    "guide2_results = []\n",
    "\n",
    "# pid별로 순차 처리\n",
    "for pid in pid_list:\n",
    "    print(f\"\\n🔄 Processing PID: {pid}\")\n",
    "\n",
    "    # 1. segment 분석 및 저장\n",
    "    ### shap_value_v?\n",
    "    #selected_segments, score_list, critical_array = segment_single_pid(pid=pid, diff_dir=\"result\", shap_dir=\"shap_value_v1\", smooth_sigma=1)\n",
    "    #update_segment_pid(pid, selected_segments=selected_segments)\n",
    "\n",
    "    # 2. 비교 함수 실행 (결과 저장 가능)\n",
    "    #comp_df = compare_result_pid(pid, base_csv=base, result_dir='result_v6_b')\n",
    "    #comparison_results.append(comp_df)\n",
    "\n",
    "    #comp2_df = compare2_result_pid(pid, base_csv=base2, result_dir='result_v6_b')\n",
    "    #comparison2_results.append(comp2_df)\n",
    "\n",
    "    #guide_df = compare_guide_pid(pid, standard_csv=base, result_dir='result_v6_b')\n",
    "    #guide_results.append(guide_df)\n",
    "\n",
    "    guide2_df = compare2_guide_pid(pid, standard_csv=base2, result_dir='result_v2_c')\n",
    "    guide2_results.append(guide2_df)\n",
    "\n",
    "    # 3. shap 개선\n",
    "    ### shap_v?\n",
    "    #improve_pid(pid, result_dir='result', shap_dir='shap_v1')\n",
    "\n",
    "#결과를 하나의 DataFrame으로 병합하고 활용\n",
    "#all_comp_df = pd.concat(comparison_results, ignore_index=True)\n",
    "#all_comp2_df = pd.concat(comparison2_results, ignore_index=True)\n",
    "#all_guide_df = pd.concat(guide_results, ignore_index=True)\n",
    "all_guide2_df = pd.concat(guide2_results, ignore_index=True)\n",
    "\n",
    "### 저장\n",
    "#all_comp_df.to_csv(\"v6_b_compare.csv\", index=False)\n",
    "#all_comp2_df.to_csv(\"v6_b_compare2.csv\", index=False)\n",
    "#all_guide_df.to_csv(\"v6_b_guide.csv\", index=False)\n",
    "all_guide2_df.to_csv(\"v2_c_guide2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bfa56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
